#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏, –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã: .docx, .xlsx
"""

import os
import argparse
from pathlib import Path
from typing import List, Optional
import pickle

# LangChain –∏–º–ø–æ—Ä—Ç—ã
from langchain_community.document_loaders import Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å Excel
import pandas as pd

class DocumentProcessor:
    def __init__(self, embeddings_model: str = "intfloat/multilingual-e5-large"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            embeddings_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embeddings_model,
            model_kwargs={'device': 'cpu'}
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        self.vectorstore: Optional[FAISS] = None
    
    def load_docx_files(self, file_paths: List[str]) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DOCX —Ñ–∞–π–ª—ã"""
        documents = []
        for file_path in file_paths:
            if file_path.lower().endswith('.docx'):
                try:
                    loader = Docx2txtLoader(file_path)
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata['source'] = file_path
                    documents.extend(docs)
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {file_path}")
                except Exception as e:
                    print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
        return documents
    
    def load_xlsx_files(self, file_paths: List[str]) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç XLSX —Ñ–∞–π–ª—ã"""
        documents = []
        for file_path in file_paths:
            if file_path.lower().endswith('.xlsx'):
                try:
                    # –ß–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã Excel —Ñ–∞–π–ª–∞
                    excel_file = pd.ExcelFile(file_path)
                    for sheet_name in excel_file.sheet_names:
                        df = pd.read_excel(file_path, sheet_name=sheet_name)
                        
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ —Ç–µ–∫—Å—Ç
                        content = f"–õ–∏—Å—Ç: {sheet_name}\n\n"
                        content += df.to_string(index=False)
                        
                        doc = Document(
                            page_content=content,
                            metadata={
                                'source': file_path,
                                'sheet': sheet_name
                            }
                        )
                        documents.append(doc)
                    
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {file_path} ({len(excel_file.sheet_names)} –ª–∏—Å—Ç–æ–≤)")
                except Exception as e:
                    print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
        return documents
    
    def load_documents(self, directory: str) -> List[Document]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        
        Args:
            directory: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        supported_extensions = ['.docx', '.xlsx']
        file_paths = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã
        for ext in supported_extensions:
            file_paths.extend(Path(directory).glob(f"**/*{ext}"))
            file_paths.extend(Path(directory).glob(f"**/*{ext.upper()}"))
        
        file_paths = [str(f) for f in file_paths]
        
        if not file_paths:
            print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {directory}")
            return []
        
        print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(file_paths)}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = []
        documents.extend(self.load_docx_files(file_paths))
        documents.extend(self.load_xlsx_files(file_paths))
        
        return documents
    
    def create_vectorstore(self, documents: List[Document]) -> None:
        """
        –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        if not documents:
            print("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
            return
        
        print(f"–†–∞–∑–±–∏–µ–Ω–∏–µ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.text_splitter.split_documents(documents)
        print(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        print("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
        print("‚úì –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ")
    
    def save_vectorstore(self, path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        if self.vectorstore is None:
            print("–ù–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        self.vectorstore.save_local(path)
        print(f"‚úì –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {path}")
    
    def load_vectorstore(self, path: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        
        Args:
            path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            self.vectorstore = FAISS.load_local(path, self.embeddings)
            print(f"‚úì –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {path}")
            return True
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
            return False
    
    def search(self, query: str, k: int = 5) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if self.vectorstore is None:
            print("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
            return []
        
        results = self.vectorstore.similarity_search(query, k=k)
        return results

def interactive_search(processor: DocumentProcessor):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ"""
    print("\n" + "="*60)
    print("–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ü–û–ò–°–ö –ü–û –î–û–ö–£–ú–ï–ù–¢–ê–ú")
    print("="*60)
    print("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
    print("-"*60)
    
    while True:
        try:
            query = input("\nüîç –ó–∞–ø—Ä–æ—Å: ").strip()
            
            if query.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥', 'q']:
                print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not query:
                print("–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
                continue
            
            print(f"\n–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'...")
            results = processor.search(query, k=5)
            
            if not results:
                print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                continue
            
            print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            print("-" * 60)
            
            for i, doc in enumerate(results, 1):
                print(f"\n#{i}")
                print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                if 'sheet' in doc.metadata:
                    print(f"–õ–∏—Å—Ç: {doc.metadata['sheet']}")
                print(f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc.page_content}")
                print("*"*60)
        except KeyboardInterrupt:
            print("\n\n–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}")

def main():
    parser = argparse.ArgumentParser(description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ–∏—Å–∫ —Å FAISS")
    parser.add_argument("--docs", "-d", type=str, help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏", default="data")
    parser.add_argument("--vectorstore", "-v", type=str, default="./vectorstore", 
                       help="–ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É")
    parser.add_argument("--load-only", "-l", action="store_true", 
                       help="–¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
    
    args = parser.parse_args()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    processor = DocumentProcessor()
    
    if args.load_only:
        # –¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        if not processor.load_vectorstore(args.vectorstore):
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
            return
    else:
        if not args.docs:
            print("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (--docs)")
            return
        
        if not os.path.exists(args.docs):
            print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.docs}")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = processor.load_documents(args.docs)
        
        if not documents:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        processor.create_vectorstore(documents)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        processor.save_vectorstore(args.vectorstore)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
    interactive_search(processor)

if __name__ == "__main__":
    main()
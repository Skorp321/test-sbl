import os
import argparse
from pathlib import Path
from typing import List, Optional, Tuple
import time
import requests

from improved_text_splitter import ImprovedTextSplitter
from text_splitter import TextSplitter

# LangChain –∏–º–ø–æ—Ä—Ç—ã
from langchain_community.document_loaders import Docx2txtLoader
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å Excel
import pandas as pd

# –î–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

class MonoT5Reranker:
    def __init__(self, model_name: str = "castorini/monot5-large-msmarco", max_retries: int = 3):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MonoT5 —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
            max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        """
        self.model_name = model_name
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞ {model_name}...")
        
        # Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        for attempt in range(max_retries):
            try:
                self.tokenizer = T5Tokenizer.from_pretrained(
                    model_name,
                    legacy=False
                )
                break
            except (requests.exceptions.ConnectionError, 
                    requests.exceptions.Timeout,
                    ConnectionError,
                    Exception) as e:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt == max_retries - 1:
                    raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                time.sleep(2 ** attempt)
        
        # Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        for attempt in range(max_retries):
            try:
                self.model = T5ForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
                break
            except (requests.exceptions.ConnectionError,
                    requests.exceptions.Timeout,
                    ConnectionError,
                    Exception) as e:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt == max_retries - 1:
                    raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                time.sleep(2 ** attempt)
                
        self.model.eval()
        self.device = next(self.model.parameters()).device
        print(f"MonoT5 —Ä–µ—Ä–∞–Ω–∫–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ü–æ–ª—É—á–∞–µ–º ID —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è 'true' –∏ 'false'
        self.true_token_id = self.tokenizer.encode('true', add_special_tokens=False)[0]
        self.false_token_id = self.tokenizer.encode('false', add_special_tokens=False)[0]
        print(f"True token ID: {self.true_token_id}, False token ID: {self.false_token_id}")
        
    def rerank(self, query: str, documents: List[Document], top_k: Optional[int] = None) -> List[Tuple[Document, float]]:
        """
        –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ (None - –≤—Å–µ)
            
        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–¥–æ–∫—É–º–µ–Ω—Ç, —Å–∫–æ—Ä)
        """
        if not documents:
            return []
        
        scores = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        for doc in documents:
            # MonoT5 —Ñ–æ—Ä–º–∞—Ç: "Query: {query} Document: {document} Relevant:"
            prompt = f"Query: {query} Document: {doc.page_content} Relevant:"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                prompt,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt"
            )
            
            # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                # –î–ª—è T5 –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å generation
                # –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å pad —Ç–æ–∫–µ–Ω–∞
                decoder_input_ids = torch.tensor([[self.tokenizer.pad_token_id]], device=self.device)
                
                # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∏—Ç—ã –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                outputs = self.model(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    decoder_input_ids=decoder_input_ids
                )
                
                # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∏—Ç—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø–æ—Å–ª–µ pad —Ç–æ–∫–µ–Ω–∞
                logits = outputs.logits[0, 0, :]  # [vocab_size]
                
                # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–∏—Ç—ã –¥–ª—è 'true' –∏ 'false'
                true_logit = logits[self.true_token_id].item()
                false_logit = logits[self.false_token_id].item()
                
                # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ softmax
                true_prob = torch.softmax(torch.tensor([false_logit, true_logit]), dim=0)[1].item()
                scores.append(true_prob)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å–∫–æ—Ä–∞–º
        scored_docs = list(zip(documents, scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if top_k is not None:
            scored_docs = scored_docs[:top_k]
        
        return scored_docs

class DocumentProcessor:
    def __init__(self, embeddings_model: str = "sentence-transformers/all-MiniLM-L6-v2", 
                 reranker_model: str = "castorini/monot5-large-msmarco", 
                 use_reranker: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            embeddings_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            reranker_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
            use_reranker: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä
        """
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embeddings_model
        )
        """self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )"""
        self.text_splitter = ImprovedTextSplitter()
        self.faiss: Optional[FAISS] = None
        self.vectorstore: Optional[EnsembleRetriever] = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞
        self.use_reranker = use_reranker
        if self.use_reranker:
            try:
                print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞...")
                self.reranker = MonoT5Reranker(reranker_model)
                print("‚úì –†–µ—Ä–∞–Ω–∫–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞: {e}")
                print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞")
                self.use_reranker = False
                self.reranker = None
        else:
            self.reranker = None
    
    def load_docx_files(self, file_paths: List[str]) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç DOCX —Ñ–∞–π–ª—ã"""
        documents = []
        for file_path in file_paths:
            if file_path.lower().endswith('.docx'):
                try:
                    loader = Docx2txtLoader(file_path)
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata['source'] = file_path
                    documents.extend(docs)
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {file_path}")
                except Exception as e:
                    print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
        return documents
    
    def load_xlsx_files(self, file_paths: List[str]) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç XLSX —Ñ–∞–π–ª—ã"""
        documents = []
        for file_path in file_paths:
            if file_path.lower().endswith('.xlsx'):
                try:
                    # –ß–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã Excel —Ñ–∞–π–ª–∞
                    excel_file = pd.ExcelFile(file_path)
                    for sheet_name in excel_file.sheet_names:
                        df = pd.read_excel(file_path, sheet_name=sheet_name)
                        
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º DataFrame –≤ —Ç–µ–∫—Å—Ç
                        content = f"–õ–∏—Å—Ç: {sheet_name}\n\n"
                        content += df.to_string(index=False)
                        
                        doc = Document(
                            page_content=content,
                            metadata={
                                'source': file_path,
                                'sheet': sheet_name
                            }
                        )
                        documents.append(doc)
                    
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {file_path} ({len(excel_file.sheet_names)} –ª–∏—Å—Ç–æ–≤)")
                except Exception as e:
                    print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
        return documents
    
    def load_documents(self, directory: str) -> List[Document]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        
        Args:
            directory: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        supported_extensions = ['.docx', '.xlsx']
        file_paths = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã
        for ext in supported_extensions:
            file_paths.extend(Path(directory).glob(f"**/*{ext}"))
            file_paths.extend(Path(directory).glob(f"**/*{ext.upper()}"))
        
        file_paths = [str(f) for f in file_paths]
        
        if not file_paths:
            print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {directory}")
            return []
        
        print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(file_paths)}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = []
        documents.extend(self.load_docx_files(file_paths))
        documents.extend(self.load_xlsx_files(file_paths))
        
        return documents
    
    def create_vectorstore(self, documents: List[Document]) -> None:
        """
        –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        """
        if not documents:
            print("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
            return
        
        print(f"–†–∞–∑–±–∏–µ–Ω–∏–µ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
        #chunks = self.text_splitter.split_documents(documents)
        chunks = []
        for document in documents:
            one_chunks = self.text_splitter.split_text_with_context(document.page_content)
            for chunk in one_chunks:
                # –¢–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ Document
                if chunk['type'] == 'content':
                    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    metadata = dict(document.metadata) if hasattr(document, "metadata") else {}
                    if 'header' in chunk:
                        metadata['header'] = chunk['header']
                    chunks.append(Document(page_content=chunk['content'], metadata=metadata))
        print(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        print("–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä, –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä—ã –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        retriever_k = 20 if self.use_reranker else 4
        
        bm25 = BM25Retriever.from_documents(chunks)
        bm25.k = retriever_k
        self.faiss = FAISS.from_documents(chunks, self.embeddings)
        print("‚úì –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ")
        retriever = self.faiss.as_retriever(
                search_type="similarity",
                k=retriever_k,
                score_threshold=None,
            )

        self.vectorstore = EnsembleRetriever(
            retrievers=[bm25, retriever],
            weights=[0.5, 0.5],
        )
        
        if self.use_reranker:
            print(f"–†–µ—Ç—Ä–∏–≤–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ {retriever_k} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            
    
    def save_vectorstore(self, path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        if self.faiss is None:
            print("–ù–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        self.faiss.save_local(path)
        print(f"‚úì –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {path}")
    
    def load_vectorstore(self, path: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        
        Args:
            path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –∏–Ω–∞—á–µ
        """
        try:
            self.faiss = FAISS.load_local(
                path, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º ensemble retriever
            retriever_k = 20 if self.use_reranker else 4
            
            # –°–æ–∑–¥–∞–µ–º BM25 retriever (–Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
            # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ FAISS
            retriever = self.faiss.as_retriever(
                search_type="similarity",
                k=retriever_k,
                score_threshold=None,
            )
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ FAISS retriever
            self.vectorstore = retriever
            
            print(f"‚úì –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {path}")
            if self.use_reranker:
                print(f"–†–µ—Ç—Ä–∏–≤–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –ø–æ–ª—É—á–µ–Ω–∏–µ {retriever_k} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞")
            return True
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
            return False
    
    def search(self, query: str, k: int = 5, initial_k: Optional[int] = None) -> List[Tuple[Document, float]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–æ–≥–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            initial_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (–±–æ–ª—å—à–µ k –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–¥–æ–∫—É–º–µ–Ω—Ç, —Å–∫–æ—Ä)
        """
        if self.vectorstore is None:
            print("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
            return []
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ—Ä–∞–Ω–∫–µ—Ä, –ø–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
        if self.use_reranker and self.reranker is not None:
            search_k = initial_k or min(k * 3, 20)  # –ë–µ—Ä–µ–º –≤ 3 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –º–∞–∫—Å–∏–º—É–º 20
            results = self.vectorstore.invoke(query)
            
            print(f"–†–µ—Ä–∞–Ω–∫–∏–Ω–≥ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            reranked_results = self.reranker.rerank(query, results, top_k=k)
            print(f"‚úì –í–æ–∑–≤—Ä–∞—â–µ–Ω–æ {len(reranked_results)} —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return reranked_results
        else:
            # –ë–µ–∑ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –±–µ–∑ —Å–∫–æ—Ä–æ–≤ (—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–∫–æ—Ä 0.0)
            results = self.vectorstore.invoke(query)
            return [(doc, 0.0) for doc in results[:k]]

def interactive_search(processor: DocumentProcessor):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ"""
    print("\n" + "="*60)
    print("–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ü–û–ò–°–ö –ü–û –î–û–ö–£–ú–ï–ù–¢–ê–ú")
    print("="*60)
    if processor.use_reranker:
        print("üîÑ –†–µ—Ä–∞–Ω–∫–µ—Ä –∞–∫—Ç–∏–≤–µ–Ω: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
    else:
        print("‚ö†Ô∏è  –†–µ—Ä–∞–Ω–∫–µ—Ä –æ—Ç–∫–ª—é—á–µ–Ω")
    print("–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ 'quit' –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
    print("-"*60)
    
    while True:
        try:
            query = input("\nüîç –ó–∞–ø—Ä–æ—Å: ").strip()
            
            if query.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥', 'q']:
                print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not query:
                print("–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
                continue
            
            print(f"\n–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'...")
            results = processor.search(query, k=5)
            
            if not results:
                print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                continue
            
            print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)}")
            print("-" * 60)
            
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n#{i} [–°–∫–æ—Ä: {score:.4f}]")
                print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                if 'sheet' in doc.metadata:
                    print(f"–õ–∏—Å—Ç: {doc.metadata['sheet']}")
                if 'header' in doc.metadata:
                    print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {doc.metadata['header']}")
                print(f"–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc.page_content}")
                print("*"*60)
        except KeyboardInterrupt:
            print("\n\n–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞: {e}")

def main():
    parser = argparse.ArgumentParser(description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ–∏—Å–∫ —Å FAISS")
    parser.add_argument("--docs", "-d", type=str, help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏", default="data")
    parser.add_argument("--vectorstore", "-v", type=str, default="./vectorstore", 
                       help="–ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É")
    parser.add_argument("--load-only", "-l", action="store_true", 
                       help="–¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
    parser.add_argument("--no-reranker", action="store_true", 
                       help="–û—Ç–∫–ª—é—á–∏—Ç—å —Ä–µ—Ä–∞–Ω–∫–µ—Ä")
    parser.add_argument("--reranker-model", type=str, default="castorini/monot5-large-msmarco",
                       help="–ú–æ–¥–µ–ª—å —Ä–µ—Ä–∞–Ω–∫–µ—Ä–∞")
    
    args = parser.parse_args()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    processor = DocumentProcessor(
        use_reranker=not args.no_reranker,
        reranker_model=args.reranker_model
    )
    
    if args.load_only:
        # –¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        if not processor.load_vectorstore(args.vectorstore):
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ")
            return
    else:
        if not args.docs:
            print("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (--docs)")
            return
        
        if not os.path.exists(args.docs):
            print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.docs}")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        documents = processor.load_documents(args.docs)
        
        if not documents:
            print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        processor.create_vectorstore(documents)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        processor.save_vectorstore(args.vectorstore)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
    interactive_search(processor)

if __name__ == "__main__":
    main()